# Weekly Journals

## Table of Contents
- [Week 1](#week-1-may-19-23)
- [Week 2](#week-2-may-27-30)
- [Week 3](#week-3-june-2-6)
- [Week 4](#week-4-june-9-13)
- [Week 5](#week-5-june-16-20)


## Week 1 *(May 19-23)*:
### Day 1:
During the first day, much of the time was spent addressing initial set-up of the two Jetson Nano devices and explaining the project. We realized only one board was installed with Ubuntu 20.04 with CUDA enabled, with the other not having CUDA and running unsupported Ubuntu 18.04. This second board will need to be upgraded to 20.04 for supported functionality and consistent environment testing, but for simplicity, we will not need to enable CUDA on it. The computer vision algorithms will be used on the Nano that has CUDA, while testing for Speech-to-Text/Automatic Speech Recognition will take place on the other board. We ran into many problems trying power on the devices, but resolved these issues once a found compatible cables and power outlets as well as switching the boards to the correct power mode. With functioning Nano devices, we will be looking to start installing the software we require for the project to begin initial testing and benchmarking. 
### Day 2: 
This day was starting to focus more on installing the computer vision libraries necessary. Many of the libraries needed were already installed, along with Python 3.8, on the system, however these are not the most up-to-date versions of the software. Ubuntu 20.04 comes out the box only up to Python 3.8 though, so for simplicity, we will start by just using the libraries on this version and upgrade only if necessary later, since all the needed libraries should work with this Python version. Ultralytics is the only library of interest in which the latest version is available on Python 3.8, which is essential since it provides support for YOLOv11 to run the optimized models. The main libraries of interest are PyTorch, TensorRT, Ultralytics (YOLO), ONNX, and potentially others, which will all be compared side by side for benchmarking once they are installed and functioning soon. The Nano devices also were having trouble connecting to the internet, however, so progress was halted until they are registered to the network or some other workaround. The libraries will be installed using a newly created virtual environment on Python 3.8, ensuring reproducible installations and library environment. Testing camera usage will hopefully follow. 
### Day 3:
During this day, the camera was tested to ensure that it properly was connected to the board and was usable, which was confirmed through shell commands to display the camera input. Initial attempts to read the camera input from the CSI input via python script with OpenCV was unsuccessful, though this was not thoroughly pursued yet. The camera has already been recognized to work, so I am less concerned with getting it to work simply in Python code. The next test came with installing and testing the Ultralytics YOLOv11 model. The model seemed to work fine and functional; it seemed as though the accuracy was fairly good but not great, but it could not be further tested more properly because the framerate was less than 1 fps. A further look at the problem revealed that the library was not utilizing the graphics card for computation because it needed a flag to be set to use CUDA; however, PyTorch is not installed on the Nano with CUDA enabled. Extensive online searching seems to reveal that PyTorch does not release pip installations or wheel files that both have CUDA enabled and are suited for the ARM64/aarch64 architecture of the Jetson Nano. I am looking to try to download the source code and compile it locally to bypass this issue, however this may end up being a large problem if there is no way to run YOLOv11 with PyTorch using graphics card computation. 
### Day 4:
Much of the day was unproductive due to the inability of the Jetson to sufficiently run the YOLOv11 pose models. Many of the models would not boot up at all when loading them in, and those that did ran at a mere ~3-5 fps, if that. Most models, even with optimized loading parameters (int8 instead of int16, half=True, device->CUDA) could not run the models very well. It was difficult to determine the source of the poor performance, since loading jtop to check system resources proved impossible due to the freezing and constant unresponsiveness of the machine. Many of the export formats of YOLO models to other frameworks are possibly incompatible for the CUDA version and architecture of the Nano. ONNXRuntime-gpu does not have a build for CUDA 10.2, meaning I don't know if this can be used. TensorRT could not be used before files were copied to the virtual environment from the original installed library, as pip would not build a wheel for it on a Tegra system. I will test tensorRT models next. I also plan to use the 'benchmark' module included with ultralytics.utils package as an additional way to compare model performance, though I suspect that these tests will not perform to desire. Another large problem discovered is that the OpenCV installed on the Jetson from Jetpack 4.6 does not have GStreamer, which is how the library interacts normally with the system's CSI camera. This means we will likely have to find an alternative to OpenCV to get input video or to locally build an OpenCV library that enables GStreamer. 
### Day 5:
Several different model types/frameworks continued to not work for YOLO with Jetson Nano, including TensorRT, NCNN (which used to work), ONNX, and TorchScript. There may be small things that change with the system as I try to install/change libraries to accomodate, which might lead to inconsistencies in if libraries start/stop working seemingly randomly. The CSI camera is now functioning in python through the use of the library gi and Gst from gi.repository. I set up logging to track ms per iteration and system resource usage for each model that works. These should show if anything becomes a bottleneck for the system; I am also personally noting what parameters I am using with each model. I was able to test the YOLOv8 and YOLOv11 models, both regular and pose versions, and so far the v11 versions do not seem to provide any computer vision results; simply just the camera output (at reduced frame rate). All models give about 5 fps roughly, so no clear winner other than that v11 seems to be giving issues that I will look into more. Playing around with the model parameters has not seemed to help too dramatically; I want to find a different model format that works to test performance with (though not many other common frameworks seem to be working right now), or possibly try a different Pose Recognition model altogether to see if other options perform well (i.e. MoveNet). 

## Week 2 *(May 27-30)*:
### Day 1:
Testing of different model types continued today on the Jetson Nano, as well as some starting testing on the Raspberry Pi 5 for the best performing models. Most models seem to run at highest around 5 fps on the Nano, including the TensorRT Engine model that would likely be the best running on the Jetson given TRT's high GPU optimization. Unfortunately, this still does not seem to compete with any of the results of the Raspberry Pi on face value, as several model types for the YOLO11-pose give frame rates of at least ~30fps, several are seemingly even much higher ~50-60 including NCNN, ONNX, and OpenVino. These types seemed to be more compressed and optimized for edge computing, and despite their lack of GPU processing optimization, the Pi still appears to contain superior hardware speeds. I will keep logging specific model metrics for each board to gauge the best option; I will also try to find a model type that works well with the Jetson Nano to see if it may still be useful. Otherwise, it seems the Pi is capable of everything or better from a performance standpoint. 
### Day 2:
Today was mostly spent for me to set up all the testing for a thorough analysis of metrics for all different YOLOv11n-pose model configurations on the Raspberry Pi 5. I finished writing a script that will take in a stock video of a person dancing to use as a constant input for testing, and feed this through the model selected while testing metrics such as system cpu/gpu/memory usage, time since last iteration/since start, fps, preprocessing/inference/postprocessing time, and accuracy. This data is all being logged to a CSV file that can later be called upon for data and number crunching if needed for generating any graphs for the paper or simply for in-depth side-by-side comparison of two different models' performances. The accuracy simply includes the text form of a tensor for the recorded confidence rates of the 17 keypoints- this was gathered from `results[0].kepoints.conf`; if this data is of interest down the line, it should not be hard to parse using a simple script. The model configurations I am primarily exporting away from pytorch/torchscript format and testing the faster frameworks of NCNN, MNN, ONNX, and OpenVINO for the Raspberry Pi. Each format gets 3 different versions of settings for base testing: the default `imgsz=640`, `imgsz=320`, and `imgsz=320` & `int8=True` (or `half=True` if int8 is incompatible); these *should* yield increasingly better performace per respective model. TensorRT does not work on Rasp Pi given no GPU, and TF Edge TPU does not work on aarch64 architecture, which both would have otherwise been formats of interest. I do not think the Jetson wilil provide any results superior to the Raspberry Pi, so I am hisitant to even spend time with it for benchmark testing. I have, however, seen 20-30 fps rates from people online using YOLO models with TensorRT on the Jetson Nano. Therefore, I will try to find performance out of the TensorRT format of YOLOv11n-pose as well as TRT-pose, but otherwise I don't plan to do too much further testing for Jetson. Today we also discussed the possibility of implemeting a "game" aspect as a feature of the project in the future, where motion tracking is used to make you phyiscaly move to control the game. A sample game was implemented for space invaders with this concept showing the user's skeleton on screen as well, with other ideas such as flappy bird being an idea for future game creation. 
### Day 3:
This day was mostly all spent carrying out the benchmark testing, where I would load a model through a script, feed it in the same sample video of a guy dancing, and collect metrics which were logged to a CSV file. An output mp4 file was also generated with the estimated pose keypoints overlaying the guy in the video, and I also generated a line-by-line breakdown using the library `line_profiler` to see where the cpu spent the most time and iterations in the code. Without thorough analysis of the graphs producable by the logging files, I was still able to find some base conclusions about model types. CNN, OpenVINO, and MNN all seem to be promising and the best performing around 20 fps, with ONNX being slightly behind, and Torchscript not even close. Imgsz of 640 produces the best visual accuracy and smoothness while tracking keypoints on the body, but produces much slower frame rates (usually ~10 fps). 320 imgsz is much better speed, and is likely the lower limit of resolution for this, as it is visible at this stage to have slightly shakey keypoints and visible innaccuracies. When going lower than that, 240 imgsz is faster at around 22-23fps but has more visible slopiness in the keypoint estimation, and lower down to 160 imgsz is very unusable, as despite being very quick at about 28 fps, there are points where limbs are completely misjudged by the model, and even points where it thinks there is more than one person from the sample video. Smaller input size leads to faster passes through the model while getting lower resolution results, both of which make sense. I also found that the `int8=True` and `half=True` parameters were completely unhelpful; in fact, there were some models that these seemed to make it actually run slower. This does not make sense to me, and perhaps these would be more helpful on a device with higher memory constraints, but on the Raspberry Pi 5 they do not seem to be useful for us. 
I had an idea for improving processesing speeds. Given that we want to decrease input resolution for faster speeds without compromising accuracy, I think we could possibly take a different approach in future development. We can use a dynamic model type: "Allows dynamic input sizes for ONNX, TensorRT and OpenVINO exports, enhancing flexibility in handling varying image dimensions." This may lead to possible speed reduction, however we can use this for cropping of the camera depending on how close/far the person is in the image, as well as on left/right/middle. Often times, a person is only taking maybe a third of the camera screen, so passing this all to the model is wasteful. We would first pass the whole input through model. Then, on the next pass, take just a cropped part of camera image based on the location of estimated keypoints + a margin for person's movements and pass this resized image through the model. We could then recrop the camera based on where the estimated position of the person is in the frame. This should allow reduced input imgsz to the model without compromising camera FOV/resolution or the keypoint accuracy of the model, since a 320 pixel imgsz is more effective if they are the true pixels from the camera rather than being scaled down from 1280x720, for example. 
### Day 4
I spent today split between two different tasks: trying to install and run trt_pose on the Jetson Nano, and implementing dynamic image cropping for feeding into the YOLO model on the Raspberry Pi 5. I continue to have difficulty getting things to work on the Jetson, as I run into issues at almost every turn. However, I there is a jupyter notebook file posted on the trt_pose github that walks through a demo of setting it up, and I have tried to modify it to fit the needs of my environment. I am close to getting it working and hope to be able to see its results on Monday. In terms of the Rasp Pi, I laid out the start for the cropping of the screen. It does not work quite yet, but I hope to iron out bugs and see if there are any apparent performance/accuracy gains from this method. I also hope to be able to start looking into and implementing pose recognizition on the Rasp Pi, such as displaying the current position it believes the person is in (such as standing, sitting, squatting, arms up/out, or anything useful). I will look to see if there are benefits of using a pretrained model on this type of data or if I should write custom algorithms to determine a person's pose. 

## Week 3 *(June 2-6)*:
### Day 1:
Today was comprised mainly of trying to make sure my project and Alex's project both work properly on the Raspberry Pi 5. With the end goal of having a basic program with all aspects running, we needed to make sure the environment was set correctly for the Pi to run everything. My python libraries were already running well, so I did not need to do any work to make sure it could run, though the Picamera package seems to need to be used through the system installation- it can't really be pip installed with Rasp Pi 5. For Alex, he brought his requirements.txt file for pip, and all installed properly except the packages simpleaudio, PyAudio, and sudachipy. These needed a Rust compiler to be installed in order to build the wheel files locally, but they installed fine once this was done. After this, we tested the running of the conversational pipeline with ASR -> LLM -> TTS, which did not seem to work at first as the LLM gave very strange responses, but we fixed this after tweaking the prompt to the model. I also created the basics for a script that is calculating the angles of all the key joints on the body (knees, hips, shoulders, elbows, neck). This information will then be run through conditional checks for what position the body is in (standing, sitting/squatting, arms out/up, etc.) as well as if they are completing reps of an exercise. 
### Day 2: 
The first part of the day was spent figuring out what was the best way to integrate our good together and get it all moved to the Raspberry Pi 5. We wanted to try to have a working script by the end of the day that could run both of our programs simultaneously, and this got accomplished. Alex and I both had our code pushed to the GitHub Repo, which allowed easy pulling on the Pi to get our test code from our laptops. Both our codes contained looping blocks, but since the ASR section had a blocking line of code that was listening to the microphone for several seconds, the two programs had to be multithreaded in order to work together. As a result, `kokoro_threaded.py` was deemed the "main" file, and the `yolo_threaded.py` file keeps the ultralytics model and computer vision processing in a separate file for organization. This file is then imported and started in a second thread, though the file can also be run by itself for individual testing. `shared_data.py` was created as a utility-type file for thread-safe sharing of data, variables, and states between the yolo processesing script and the user interaction script. This will allow for communication between the user of the program and the LLM to also control the exercise routine, as well as letting the computer vision program inform the TTS if an the user should be alerted of something. In keeping the separate functions in different folders, we had to make use of relative importing and create dummy `__init__.py` files for each diractory to declare them as packages that can be imported in code. To run the program, the new command is `python -m ConversationalAI.kokoro_threaded.py` to declare to the compiler that it is to be used as a module. I feel that this day was a good step towards having a functional system of both the conversational and imaging sides. I plan to continue testing these two together as well as working with Alex to come up with additional simple exercises and requests the user can participate in through the use of voice commands.  
### Day 3:
I continued today to make good progress towards having a program that can perform and analyze different exercises by the user, including lunges, arm raises, and squats. These were all used as examples that can then be later prompted by the user through the ASR to start an exercise. I also made many basic checks for these exercises as to whether or not the user was performing the exercise correctly and with good form, which otherwise the screen will display a message alerting the user of how to fix their posture. Alex and I then worked to make the ASR be able to communicate and start/stop the yolo thread via voice commands with the thread-safe data sharing. I will be looking tomorrow to try to continue developing more exercise options, as well as brainstorming how we want to continue our development of the interface of the program. 
### Day 4:
Today also made great progress towards making sure the Chatbot and the Pose models are working well together. By the end of the day, we were able to put together a much more solid foundation of the program that was passing almost all the data we planned on using between both the Chatbot and the Pose models. In addition, I have also finished modularizing much of the small checks on every exercise for static body parts, meaning that a new exercise can be created by just adding together different criteria from the different body part functions that are defined. Together, these will ensure an exercise is performed with good form and not incorrectly. These issues are all sent to the chatbot, which are communicated to the user so they can address the issue. There is still much testing needed for this to work more fluidly, and I plan to search for bugs tomorrow. I am quite happy with the place we are with how well the two threads are now working together seamlessly. Alex also implemented a potentially faster LLM for the chatbot. 
### Day 5
I spent the most of the day working out large issues with detecting when a user was performing one of our four current exercise options incorrectly. This included fix wrong logic as well as adding in new features to account for which direction the user is facing. Currently, this still needs to be a default value hardcoded, but in the future it should be something that is either specifiable by the user or that is dynamically detectable by the system and have it adapt to the user's facing direction. For the most part now, these exercises are pretty solid and are able to be completed by a user on command. We plan to implement many new exercises in the future geared towards rehab for certain audiences, such as post-stroke patients and the respective associated exercises. Next week I am looking to also implement flexibility for the user's apparent Range of Motion- if the system detects the user consistently not reaching the target min/max angle ROM, then it will prompt for if they want to adjust the workout to fit these constraints. I also would like this to be saved somehow for the user *per exercise*, meaning each exercise has its own adjusted ROM, though this may end up being a temporary implementation if at all, as this would be something saved to the user's database profile in the future. Another issue I have consistently run into is the fact that the YOLO model will default unknown/unconfident keypoints to position (0, 0). This is not too hard to get around by simply ignoring that location, though it makes calculating angles and correct form trickier with these values. Ideally, if there was a way to make a prediction on where the true location off-screen of this value is, that would be best for giving approximate values, although I also don't know that this is really necessary as the user should be instructed to be on-screen regardless. Nonetheless, I may look into pursuing a similar approach if ignoring the values ends up becoming an issue in the future. 

## Week 4 *(June 9-13)*:
### Day 1:
Today I spent basically the whole day looking over online resources, examples, and documentation on the AI Hat for Hailo8. I am trying to integrate its functionalities in with the scripts that we already have running on the YOLO models with ultralytics. However,the Hailo8 scripting is all using asynchronous callback functions, so having realtime passing of the model output to the script logic and to the screen is tricky to figure out. Looking online seems to reveal a lot of trouble to use synchrnous execution instead of callbacks, so I will continue to try to implement our code into the format the callback function needs, mirroring the example code and limited documentation posted online. 
### Day 2:
Continuing off of yesterday, I spent the day exploring how to use the hailo python libraries for the beneifit of our project and how to implement our code. Ultimately, getting the fastest possible performance through hailo would be the goal, however a lot of frustration and struggle makes me think it may not be worth it. It is possible I am misusing or mosconfigured the hardware/model/files, but when simply using the example Raspberry Pi 5 code posted on the Hailo github page, the program is inconsistent on whether it freezes up or not. Additionally, there is an integrated call to create an OpenCV window from within the library that I am unable to control, however I can create my own window in addition that produces visual output from the callback function. I probably could fish through the library files and find the code that creates the opencv window, but since the program already behaves inconsistently, I have decided it likely is not worth the trouble at my current level of knowledge of the problem; maybe this can be revisited in a long time once again in a search for performance gains. Instead, I have a working snippet of code that implements synchronous threading of the model inference and all extra program code within a loop, without worrying about library callback functions. This seems to run slower than the async version, but seems to also run faster than not using it at all. I got fairly fast frame rate running the yolo8s_pose model, so I want to try to compile the yolo11n_pose model (using 320px image size also) to a hef file to match the model type we are using in the codebase already. This will give a more proper direct comparison of speed using this model, and this model type will likely be the fastest overall. 
### Day 3:
Today was somewhat more productive as it we got through a major issue Alex was having when running the code on the Raspberry Pi 5. There was an issue for this past week where the code would not run despite it working on laptops, and there was not really any indication or errors as to what the problem was. Extensive googling led to us suspecting that it was due to opencv windows trying to be spawned on a secondary thread (not main thread), which seems to be problematic. We ended up fixing this by passing an instance of a queue object through to the yolo thread, which then puts all infered images onto that queue for the main thread to show itself. Additionally, the conversational ai was moved to its own thread as well, as this tends to block up the main thread and prevents it from being able to display the opencv window fluidly. This also allows for any other miscelaneous logic that needs to be implemented into the main loop. I am still continuing to look into why Hailo is inconsistent with fetching its gstreamer video feed. 
### Day 4: 
I have continued looking into how to use synchronous inference with HailoRT. I am getting slowly discouraged, as there is also a lacking of documentation on the method I am using. I am attempting to use the hailo_platform library, which now seems to not be very document or supported anymore as no more examples really use it. Despite that, this seems to be one of the only logical ways for me to implement it. 
### Day 5:
I have gotten prediction model working on the hailo, but not yet pose modeling. The format is not postprocessed through the combiled hef file, so I have to figure out the output format and postprocess it myself. This seems not too bad, but from my attempts there has been nothing to come of decoding the output non-maximimum suppression format. I am trying to parse this myself at this point, though I am concerned it may not be functional with the precompiled hef files given from Hailo. 

## Week 5 *(June 16-20)*:
### Day 1:
More time has been put into determing how to get code to function correctly with the YOLO pose model on the Hailo board. The biggest issue observed so far is that the precompiled hef file for the pose model does not output the same format or information as the yolo detection model (ignoring the extra keypoint info). The detection model has one output layer and gives very clean, organized data of bounding boxes, confidence scores, object classes, etc. However, the pose model has 9 different output layers that are all outputed to the user as a raw output, not having gone through non-maximum suppression or any postprocessing yet of any kind. This has made it very difficult to handle. I finally found a working solution involving asynchronous multiprocessing to pass info from image input -> preprocessing -> inference -> postprocessing. It does not appear to run quite as fast as hoped though, only hitting ~19fps, so I keep looking into why this is. 
### Day 2: 
I have made substantial progress to find out the problems behind performace. Firstly, I finally managed to create a synchronous version (single thread) of the multiprocessed solution for YOLO pose model inference on the Hailo. This was very stressful, but made debugging and tracking performance extraordinarily easier. In this script, postprocessing functions are being run to parse and filter the output of the 9 output layers from the pose hef file into keypoint locations on the screen. However, the performance of this model is considerably slower than expected or hoped due to this very fact. I used the very useful `line_profiler` cli module for python to profile where cpu time is being spent, which is now possible to track everything since it is all synchronized on the main thread at once. The results show that inference time (for yolov8m_pose.hef estimation) takes ~20% of program time (which is being blocked while waiting for Hailo to compute), but the postprocessing for the model done on the CPU is takes a whopping 50% of the program time. The Hailo-8 is also only being utilized at about 16%, and the average framerate is 20fps. In comparison to the sample detection model code using yolov8m.hef, the inference time takes closer to 67% of the program time, and uses about 65% of the Hailo-8 board - all while achieving a higher 30fps. This is much more ideal, since the main load is being offloaded onto the hailo, which could then be made asynchronous and multithreaded in order to free up the cpu instead of blocking the main thread (the synchronous is of course to only test functionality). However, the issue with the pose model is that a large portion of heavy computation is still being done on the cpu via postprocessing instead of the hailo board, meaning there is much more work on the cpu and less ability to multithread for performance gains. The current solution for pose estimation with the Hailo board is, as a result, *drastically* slower than the detection model running on the Hailo-8 AND the pose estimation model NOT running on the Hailo-8 (only running on Rasp Pi cpu). I believe the step forward for this is to look into custom compiling the yolov8*_pose.pt -> onnx -> hef, assuming this is documented well enough to be done. I hadn't originally thought this would be necessary given the precompiled hef file provided by Hailo for the pose estimation, however the model they compiled does not seem to allow enough computation to be done on the hailo board from what I can tell. 
### Day 3:
I have started organizing my files to implement the hailo chip processing into the pipeline for the ARISE system. I am basically mirroring everything already present in the current yolo_threaded file, except offloading onto the hailo when the inferencing is necessary. I am also starting to add in the timing to track the average time across the preprocessing, inference, and postprocessing stages of computation, as well as a fourth category for tracking all the other things happening in the thread (displaying window, updating flags, checking workout state and reps, etc). If these extra items are not relavent in the comparison of pipelining then they can be simply ignored, as they are technically constant beside the method of inferencing. There may be a problem measuring postprocessing on the raspberry pi, though, since the output of the model is essentially already postprocessed. Regardless, the overall time comparison per pass should still provide what we are looking for. I hope to have this completed tomorrow with all data collected. 
### Day 4:
Today I finished taking notes for the average times of Raspberry Pi 5 pipeline processing time vs the Hailo when running inference on the yolov8-pose model. I was able to confirm prior hopes that when running this same model, the Hailo does in fact have around 10x the processing speed with the accelerated inferencing. The RP5 averages about 0.476 seconds per inference and 0.001 for post-processing, while the Hailo takes 0.0266 for inference and 0.0302 for post-processing (pre-processing is negligible in both - non-existant in RP5 and 0.005 in Hailo). This means while RP5 does take much less time on postprocessing, this is insignificant compared to the losses on inference time. After confirming, I worked to create a script that mimics the current yolo_threaded.py script named hailo_pose_threaded.py, so that the script may be run standalone and part of the whole in the same way the original was by setting the new import flag in stream_test.py. A note to make is that given the larger computation in postprocessing using the Hailo, currently the higher benefits of it are going to be from using these larger models (i.e. yolov8m instead of yolov8n). For example, the model currently run on RP5 standalone is the yolo11n-pose openvino imgsz 320. If we were to take this model (size nano, imgsz 320) and compile to run on Hailo, I suspect that it may not run faster due to the fact that the RP5 runs it at around 30 fps, which is comprable the Hailo's postprocessing step alone. However, this no longer seems like something to worry about - the Hailo runs ~13 fps on the yolov8m-pose and ~16fps on yolov8s-pose, which is a sufficient level to run at given the higher accuracy of the model. 

## Week 6 *(June 24-28)*:
### Day 2:
Today I found that there is a memory leak issue with the exported model I had been using, which was OpenVino format. Instead, I switched back to using the NCNN format, which seems to actually have a slightly higher frame rate on average as well, though not by much. I am unsure of the cause of the memory leak, but it is something internal to the model inferencing that never releases memory with the OpenVino format, so I don't think this is a practical option anymore. In thinking about model optimization, my past idea about sending a cropped image into the input of the model no longer seems like a valid option since the precompiled hef file is 640x640 input size, unless we were to grab camera input at the maximum size, such as resolutions of 1920x1080. I am unsure if this would provide any noticable accuracy increases, however. 
### Day 3:
I added a got a few more data points from different models to add diversity to the collected pipeline latency for Hailo vs RPi5. I was able to spend a fair amount of time tinkering with the Dataflow Compiler as well, and basically deduced that compiling the nms postprocessing onto the Hailo8 is not really possible, at least not without extreme expertise of the Hailo. It is not a supported feature, and from what I can tell, its technically not supported on the object detection models either. I opened a post on the Hailo community forums where I learned that the nms_postprocessing is still technically being done on the CPU for the yolov8m.hef model, it just happens to be done through the Hailo environment. As a result, there is not really any perceived benefit of it, as this model likely is falling victim to the same problem where postprocessing ends up taking up a large amount of the actual compute time. Unfortunately, I don't know that there is really a way to get around this at this point - The Dataflow Compiler seems too much of a headache to try to figure out for improving computation speeds. I am working to improve overall pipeline speed by queueing the output of the hailo, but I don't expect this to be a super high computation gain. 
### Day 4:
I seemed to get a working solution that individually threads the inferencing of the Hailo processor separately from the postprocessing of the output. However, this seems to not give any faster of results compared to the single thread counterpart. The problem seems to stem from the Queue taking around 30 ms to return from the `get()` function to transfer the output data between the Hailo thread and main thread. This might have to do with the large amount of numpy arrays that are passed per call to the queue, though I have no evidence to support this. I tried to not use queues and instead simply use a shared state to transfer the data as before with pose_threaded & conversationalAI (which should be thread-safe), but there seems to be some sort of inconsistent freezing of the program related to `cv2.show()`. This is running only on main-thread, so I am unsure why it doesn't work most the time and occasionally does. I also had to sleep the main thread for a little otherwise `cv2.imshow` would freeze - I don't really understand any of it. 
In addition to all of this, I did test out the Whisper model on the Hailo board, and it does seem to somewhat work on processing speech as a prerecorded chunk after the person is done talking. However, there is not a premade solution for streaming, and it doesn't seem to easily support it at the moment without some good custom work. 

