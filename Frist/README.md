# Weekly Journals

## Table of Contents
- [Week 1](#week-1-may-19-23)
- [Week 2](#week-2-may-27-30)

## Week 1 *(May 19-23)*:
### Day 1:
During the first day, much of the time was spent addressing initial set-up of the two Jetson Nano devices and explaining the project. We realized only one board was installed with Ubuntu 20.04 with CUDA enabled, with the other not having CUDA and running unsupported Ubuntu 18.04. This second board will need to be upgraded to 20.04 for supported functionality and consistent environment testing, but for simplicity, we will not need to enable CUDA on it. The computer vision algorithms will be used on the Nano that has CUDA, while testing for Speech-to-Text/Automatic Speech Recognition will take place on the other board. We ran into many problems trying power on the devices, but resolved these issues once a found compatible cables and power outlets as well as switching the boards to the correct power mode. With functioning Nano devices, we will be looking to start installing the software we require for the project to begin initial testing and benchmarking. 
### Day 2: 
This day was starting to focus more on installing the computer vision libraries necessary. Many of the libraries needed were already installed, along with Python 3.8, on the system, however these are not the most up-to-date versions of the software. Ubuntu 20.04 comes out the box only up to Python 3.8 though, so for simplicity, we will start by just using the libraries on this version and upgrade only if necessary later, since all the needed libraries should work with this Python version. Ultralytics is the only library of interest in which the latest version is available on Python 3.8, which is essential since it provides support for YOLOv11 to run the optimized models. The main libraries of interest are PyTorch, TensorRT, Ultralytics (YOLO), ONNX, and potentially others, which will all be compared side by side for benchmarking once they are installed and functioning soon. The Nano devices also were having trouble connecting to the internet, however, so progress was halted until they are registered to the network or some other workaround. The libraries will be installed using a newly created virtual environment on Python 3.8, ensuring reproducible installations and library environment. Testing camera usage will hopefully follow. 
### Day 3:
During this day, the camera was tested to ensure that it properly was connected to the board and was usable, which was confirmed through shell commands to display the camera input. Initial attempts to read the camera input from the CSI input via python script with OpenCV was unsuccessful, though this was not thoroughly pursued yet. The camera has already been recognized to work, so I am less concerned with getting it to work simply in Python code. The next test came with installing and testing the Ultralytics YOLOv11 model. The model seemed to work fine and functional; it seemed as though the accuracy was fairly good but not great, but it could not be further tested more properly because the framerate was less than 1 fps. A further look at the problem revealed that the library was not utilizing the graphics card for computation because it needed a flag to be set to use CUDA; however, PyTorch is not installed on the Nano with CUDA enabled. Extensive online searching seems to reveal that PyTorch does not release pip installations or wheel files that both have CUDA enabled and are suited for the ARM64/aarch64 architecture of the Jetson Nano. I am looking to try to download the source code and compile it locally to bypass this issue, however this may end up being a large problem if there is no way to run YOLOv11 with PyTorch using graphics card computation. 
### Day 4:
Much of the day was unproductive due to the inability of the Jetson to sufficiently run the YOLOv11 pose models. Many of the models would not boot up at all when loading them in, and those that did ran at a mere ~3-5 fps, if that. Most models, even with optimized loading parameters (int8 instead of int16, half=True, device->CUDA) could not run the models very well. It was difficult to determine the source of the poor performance, since loading jtop to check system resources proved impossible due to the freezing and constant unresponsiveness of the machine. Many of the export formats of YOLO models to other frameworks are possibly incompatible for the CUDA version and architecture of the Nano. ONNXRuntime-gpu does not have a build for CUDA 10.2, meaning I don't know if this can be used. TensorRT could not be used before files were copied to the virtual environment from the original installed library, as pip would not build a wheel for it on a Tegra system. I will test tensorRT models next. I also plan to use the 'benchmark' module included with ultralytics.utils package as an additional way to compare model performance, though I suspect that these tests will not perform to desire. Another large problem discovered is that the OpenCV installed on the Jetson from Jetpack 4.6 does not have GStreamer, which is how the library interacts normally with the system's CSI camera. This means we will likely have to find an alternative to OpenCV to get input video or to locally build an OpenCV library that enables GStreamer. 
### Day 5:
Several different model types/frameworks continued to not work for YOLO with Jetson Nano, including TensorRT, NCNN (which used to work), ONNX, and TorchScript. There may be small things that change with the system as I try to install/change libraries to accomodate, which might lead to inconsistencies in if libraries start/stop working seemingly randomly. The CSI camera is now functioning in python through the use of the library gi and Gst from gi.repository. I set up logging to track ms per iteration and system resource usage for each model that works. These should show if anything becomes a bottleneck for the system; I am also personally noting what parameters I am using with each model. I was able to test the YOLOv8 and YOLOv11 models, both regular and pose versions, and so far the v11 versions do not seem to provide any computer vision results; simply just the camera output (at reduced frame rate). All models give about 5 fps roughly, so no clear winner other than that v11 seems to be giving issues that I will look into more. Playing around with the model parameters has not seemed to help too dramatically; I want to find a different model format that works to test performance with (though not many other common frameworks seem to be working right now), or possibly try a different Pose Recognition model altogether to see if other options perform well (i.e. MoveNet). 

## Week 2 *(May 27-30)*:
### Day 1:
Testing of different model types continued today on the Jetson Nano, as well as some starting testing on the Raspberry Pi 5 for the best performing models. Most models seem to run at highest around 5 fps on the Nano, including the TensorRT Engine model that would likely be the best running on the Jetson given TRT's high GPU optimization. Unfortunately, this still does not seem to compete with any of the results of the Raspberry Pi on face value, as several model types for the YOLO11-pose give frame rates of at least ~30fps, several are seemingly even much higher ~50-60 including NCNN, ONNX, and OpenVino. These types seemed to be more compressed and optimized for edge computing, and despite their lack of GPU processing optimization, the Pi still appears to contain superior hardware speeds. I will keep logging specific model metrics for each board to gauge the best option; I will also try to find a model type that works well with the Jetson Nano to see if it may still be useful. Otherwise, it seems the Pi is capable of everything or better from a performance standpoint. 
