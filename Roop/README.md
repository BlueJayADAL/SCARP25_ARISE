# SCARP25_ARISE - Weekly Journals

## Table of Contents
- [Week 1](#Week-1)
- [Week 2](#Week-2)
- [Week 3](#Week-3)
- [Week 4](#Week-4)
- [Week 5](#Week-5)




## Week 1:
### Day 1:
Focused on getting a deeper general understanding for the project and its intended direction as well as a better feel for the features that we want implemented for this summers research. Additionally division of work to make sure that there as an equal distribution for each of our ecxpereince levels and comfortability with the different tasks at hand. Aaron has a better understanding of hardware than me and I plan on learning from him these first couple of days to better understand how to work with hardware. The main focus for today was to get the Jetson Nano devices to run and connect to a monitor for display. To do this we had to troubleshoot power issues as well as make sure that the pin outs of the SDK are working as intended with the diffferent attachments such as the fan for cooling the GPU. Additionally once able to diplay the information from the devices onto a monitor we were then able to check the software capabilities of the SDK's as we have to ensure the drivers and softwares installed are up to date and compatible with the models that we intend on implementing. For instance the Jetson Nano device from seed studio was running on ubuntu 18 which is outdated for most software packages. 

### Day 2: 
Starting day 2 the focus was updating Ubuntu as well as increasing the amount of swap space, to prevent memory errors when running out of RAM. While attempting to update Ubuntu, I had to connect the Jetson Nano to the internet and through this process realied that the decives had to be registered by the campus to be used on the internet. Therefore even with a wired connection we werent able to access the internet to get the necessary files needed for updating Ubuntu. Due to this, I started a focus on how we are going to incorperate text to speech into our system. Today I did research as well as implemented test files using Vosk, a text to speech model based on neural networks, and was able to have test files for both microhpone input as well as from an audio file such as MP4. Specifically Vosk models work with .wav files but the majority of examples that I found were converting MP4s to wav and then predicting on the data to determine what is being said. The microphone input is what is more suited towards our project, as it is offline speech to text deciphering, though using the microphone makes it hard to test edge cases like accents and background noise. Therefore I spent a good amount of time also just looking for audio files that either had background noise, making it harder to understand the words, or someone with a thick accent speaking. Doing this to then go back and test on the audio files to see the threshold at which background noise becomes too much, or where an accent is too thick, as well as different approaches to mitigate these.

### Day 3:
Today I wanted to take my focus back onto the Jetson Nano device as Aaron was able to connect his device to the wifi early on in the day. While Aaron was working on connecting his device to the internet, I worked on applying different english models for Vosk that we could use for Speech to text recognition. The intention of using all the latest available english models was to find which model worked the best for audio that may have an accent or background noise during its processing. This is due to the fact that many people have varrying accents as well as the background noise in different enviornments vary, and in the instance that someone has a thicker accent or lives in a busier, louder area, we wouldnt want our product being unintentionally discriminitory for those populations. I found that the larger the model the more time it would take to startup the model, as there is more information that needs to be loaded, though the larger models tended to struggle with audio that had an accent present. This could be due to the fact that Vosk models are Nueral Networks which have a tendency to overfit to their training data. Therefore having a larger model of a Nueral Network, which already has a tendency to overfit, could lead to even more overfitting errors. In the case of the audio files I was using for testing the larger models tended to struggle with larger words due to accents, and tried breaking them down into smaller easeir to determine words, that although may sound the same or similar to what was intended just gramatically wouldn't make sense. When it came to testing background noise though, there seemed to be an inverted bell curve for what could be called accuracy. For the audio with background noise I more so determined the success by how well the models were able to pickup any words that were said over a loud background. In this case the smaller models and the larger models were able to pickup on generally more words while the models in the middle were less likely to pickup words being said. Overall leading to my determination that we should use the Vosk's english graph model for implementation as it has a balance between accruacy, detection, and startup speed that would be realistic for our project's intentions. Finally after working on model selection, I attempted to connect my Jetson Nano to the internet and seemed to be able to. Once one the device I tried installing the software update for Ubuntu 20.04 though when the system rebooted the GUI was no longer working. I spent the rest fo the time trying to fix te GUI and may have to revert the software update to fix the packages that may have been corrupted.

### Day 4: 
Today I wanted to focus on trying to fix any packages or problems that had occured from attempting to update my Ubuntu to 20.04. Once the update was finished and the system rebooted there was an error where the system wasn't able to access address 0x50 which is thd address for the information stored in the EEPROM, to which this wouldnt allow anything to process. Once this was found out I had determined that it was necessary to reflash the device in order to revert any corrupted packages or files that were no longer compatible. Additionally I wanted to do extra research into speech-to-text models, and attempt to implement NVIDIA's Jarvis ASR for speech comprehension. This is due to the fact that we are using Jetson devices built with NVIDIA's parts, therefore the models may be more optomized for the devices that we are intending to use. I also wanted to research the implementation for a Deep Seek model for the use of conversational AI. This is because if we are processing the words spoken into text, we then have to have a model process that text to understand the text, and to do this we plan on using deep seek which is an open sourced generative AI. Understanding this model and how it should handle input can help with the decision between the speech-to-text models as one may be better suited for the processing of text. Overall this is to keep the flow of infrmation as quick and seemless as possible to reduce the amount of latency that would be in this offline system. 

### Day 5:
Starting off the day I wanted to pu more research into the possibility of using Jarvis/Riva from NVIDIA's documentation. Through doing this I was able to understand that the higher demand for storage and system requirements for Jarvis are necessary due to the fact that Jarvis has inherent NLP and TTS models on top of the base ASR model. This means that not only does the model diest human speech but it also can process it and spit out an output which is also audio. This is pretty much identical to the conversational AI concept that we have been planning on implementing though wich much steeper requirements. Therefore I started myresearch into a full pipeline for using Vosk to process words to text, and then a Llama LLM to process the text that is generated and then a TTS model to then output the LLM's response to the user. For testing I am using the smallest possible models to avoid any problems that could be encountered through system requirements as well as time taken downloading the models locally. While working on implementing all three of the models together I kept running into errors resolving packages and dependencies due to the fact that a lot of the packags needed werent already on my system, and as I tried to install them they would need otherpackages that I also do not have. Additionally the final problem I ran into was that I needed to change my python enviornment as Llama models cannot be loaded on Python if its using Anaconda to run python, therefore I had to change the way in which my system uses python as well as re-install all the packages that I already needed for my python before. I have yet to finish the implementation due to running into errors though will breakdown the pipeline to test peice by peice next time.

## Week 2:
### Day 1:
Today I focused on implementing a complete virtual enviornment for the conversational AI, as it was running into versioning problems the other day. Once I had done more reasearch, and reviwed posts on Stack Overflow I found that the most optimal version of Python was 3.11 for both the TTS and Llama-cpp for loading LLM models. Once I had switched my verions of python as well as installed the necessary packages in the virtual Enviornment I was able to run through laoding all the mdoels with no issues. Once the models began to load I was noticing some peculiar tendencies as well as outputs from the program. In my first implementation, the audio was output through the Media Player application on my windows device. This wouldnt be a good practice to have for testing as we plan on implementing these models into a SDK which is not a Windows machine and thus I installed the Pydub and simple audio packages to handle the outputting of audio from the device as to not need to use any external apps outside of the python code. Additionally sometimes when the model would output its text,  the text would have formatting assigned to the output, which would lead to a different display in the models normal API though in our case was only returning weird ouputs. For instance I prompted the model to solve a math equation and once it was done with the formula and stpes tried to box the answer using a format of "boxed12345", Im assuming trying to put a box around the value of "12345". To handle this so that the TTS model doesn't crash or give an output that isn't gramatically correct or makes sense, I added an additional function that uses regular expression to parse the text form the output of the LLM to remove any irregular characters that wouldnt make sense in human speech. In preparations to try and compare different LLM's and TTS models, I added timestamps for the start and end of each model when it is prompted or given an input, to help determine which model is the most effecient. Although the speed at whcih the information is processed is not the only metric, as reasonability and accuracy are also the main focus of the models to which they also have to be compared.

### Day 2:
For the focus of today I wanted to work on getting a better understanding on the dependencies and the viability of different TTS models to improve personability and speed. Currently the TTS model that I was using was more so jsut the only one that I was able to get working, though I have been able to find more documentation and resourses like stackademic and some of their command line commands that allow me to get a better understanding of what came with the package I have installed. My focus was to implement a model that could both emulate human speech patterns, making the spoken audio to sound more human like, while also reducing the amount of time taken to also process the text. For the personability, I attempted implementing Xtts_v2 which is Coqui's latest model for TTS though even after fixing issues with install dependencies with torch, there were deeper libraries within the Xtts model which were causing errors that I wasn't able to resolve or understand, leading to my choice in testing the your_tts model. This model is similar to previous TTS models although allowing you to use a wav file for the model to breakdown and use to recreate the audio for the text that you are parsing. Although this model is more complex than the base model I was using, it was still processing the text much faster. Although the model, speedy-speech, was by far the quickest model to process any text that I had given it. This model focuses on getting as close to real-time processing as possible by processing the data in a entirely different way than the other models. Firstly, all the models break down the text input into multiple scentences, processesing them one at a time as to not go over character limits for the TTS model, though speedy speech takes this a step further by convertign the text to the Phonetic Alphabet. Transforming text to this allows for a more robust understanding of the noises that are supposed to be made from the words that are written. Therefore allowing for the model to be more generalized as specifically in the english language we have many words that sound exactly the same but are spelled differently and this model only cares about the sound its making not necessarily the grammer behind it. Finally I've also started to put more research into the different types of LLM's that could be implemented for the project to help reduce the amount of time producing a output for a prompt. As there are different types of models focused on more niche aspects of conversation, some models are trained specifcially for chatbot purposes while others are more of a coding assisant and others could be intermediate models, have little training done, or base models, no fine-tuning. Overall I know that the number of parameters, usually denoted by a b(billions), needs to be reducd as that is the main factor in time complexity for machine learning models as they have to run through all the parameters for the data that they are processing. 

### Day 3:
The focus of today was shifting the use of TTS models form the TTS package in python from Coqui to the Kokoro-82M model, which is a quicker, more humanoid model. Through the article that Dr.Li had given me, I was able to recognize that since Kokoro had already been running locally for the person who had written the article, that it would be optimal to implement for our purposes as we already know it would be compatible with our hardware. Additionally, when testing the model, it was processing as fast as the quickest TTS models from the TTS package while also retaining the more humanoid speech. Implementing Kokoro was a little odd as they had multiple available versions of the package such as Kokoro and Kokoro-onnx. I found little documentation that was actually working for the implementation of Kokoro-onnox leading me to the Hugging Face repository for the base Kokoro package. Following the focumentation I was able to use a test model to actually hear what the model sounded like, while also downloading all necessary model files to my system. Once all the files were downloaded I was then able to locally run the TTS model, allowing for implementation into my Chatbot file. Firstly I noticed that it was processing the text the same was as the Speedy-Speech model from Coqui, by transforming English text to the Phonetic spelling of the words for easier determination of the sound, while also breaking the text prompt into more digestable chunks. Although a key difference I noticed was that the Kokoro model was trying to output the audio is it processed the text chunk by chunk, leading to the overall output to have odd breaks between sentences at times. Though this may seem wrong, I have the belief that implementing threads to handle this process could allow for the model to perform translation to audio even quicker, though I do not know the feasability of this in the context of use on our raspberry pi. I plan on testing more things out with threads in the coming days to decrease the overall response time of the chatbot for as close to real-time processing as possible. 

### Day 4:
Today I focused on implementing the use of threads to process the text that would be coming from the LLM model. This is becasue when I was using the Kokoro TTS model, I realized that it was breaking down the large paragraphs into digestable chunks for the model to handle easier, therefore one woule be able to process these chunks in parallel to further decrease the time it takes processing the data. To do this I implemented a Producer and a Consumer where the Producer in the speak function, produces the audio files that will be played by the device, where the consumer makes sure that the audio files are in the right order as well as all processed as to not ave any delays between the files as well as to aviod the text getting played out of order and not making any gramatical sense. Additionally I spent some time today looking into the documentation of LLama-cpp-python and the functions that I was using from the package to better understand the parameters. When loading the model into the project, theres a parameter allowing me to specify the number of threads that I may want for the model. Meaning that I could also implement threads potentially into the LLM to increase the processing speed. Additionally when generating a response there are even more parameters which could be useful for our purposes. Such as the streaming parameter, where if True will produce the tokens from the model as they are generated. Theoretically, I would wish to implement streaming so that the model can produce the tokens and that the TTS model can process those tokens into audio as they are produced from the model. Essentially leaving no down time between the text being produced and then the audio for that text playing. To which the audio can continue playing as more and more text is generated and processed. Currently I am working on implementing a working version of this, but the audio keeps coming out choppy/ with odd delays between different tokens. 

## Week 3:
### Day 1:
This weeks focus is on getting both pose recognition and the conversational chat bot running on a script at the same time on the raspberry pi. Our goal is going to be havving range of motion detection and tracking for the arm of a user, as well as having a breakdown of the exercise after by the convesational AI. To do this, today we focused on making a virtual enviornment on the raspberry pi that has all the necessary packags for both Open Pose as well as the ASR, LLM, and TTS dependencies needed for my chatbot. I took my virtual environment that I was working in and piped my pip list into a requirements document that could be easily parsed for the installations on the Raspberry pi. While going through the text file, some of the packages were not supported/couldnt be installed correctly due to differences in the hardware and the internal packages that are already used by the hardware. Such as the software Rust beeing needed to install the SudachiPy, as well as needing the portaudio dependency and the libasound dependency for pyaudio and simpleaudio. Once the virtual enviornment was setup, I then went to testing my previously working chatbot to make sure it was all compatible on the raspberry pi and to check if there was additonal prepping needed before the script would work. Luckily the file was able to run with no error, but the LLM model was giving extremely odd responses all of a sudden. I went back to the hugging face repository to check if there were any minor changes that I could make and attempted to use a specific format for prompting that was being used in example code. This prompting format has 3 different sections, a system prompt, user prompt, and asistant sections. The system prompt had different types of guidelines that the response should adhear to while the user prompt was the actual question that the LLM is asked to respond to. The assistant section, I believe is there for the LLM to know that the prompt is over, and that it is its job to complete the response as the assistant. Once this prompting format was put into place, the responses became much more accurate and reasonable, as well as processing much faster. Tommorrow I hope to attempt running both the pose calculations as well as the chatbot to test the capabilities of the raspberry pi and to see if there are any further optimizations that need to be done for both features of the project to be implemented and ran at the same time. 

### Day 2:
Today me and Aaron wanted to work on running both the Ultralytics open pose as well as the conversational chatbot on the raspberry pi. This was to see if there were any conflicts that running both of our files at the same time would run into though there was little to no problems with the running. To have the files both running at the same time we decided to run the open pose model on its own thread as it calculates the angles of the body as well as having a shared state file, that allows us to push and pull data from the user as they are performing their physical acivity. This idea is intended to keep everything in a discrete location therefore we know exaclty where to look when working on any of the files for documentation, as any information that would be needed by both the chatbot and openpose would be pushed to the shared state and could be handled in their own way in their own files. This additionally gives me and Aaron both the freedom to continue working on the pieces that we have developed on our own before coming together to combine them. Finally towards the end of today I attempted to start implementing an audio streaming funciton, so that instead of the file listening for a determined duration, it would intead listen until the user starts talkng and keep track of what the using is saying until completion, procedurally generating the text as it is spoken. Doing this allows for more seemless interaction between the chatbot and the user as well as the potential for a more interactive stop and pause, as currently the user has to wait for the end of that recording period for the script to know when to stop or wait. Additionally we had the idea of having the thread of the Pose detection run on a thread that is asleep until the actual activity or motion is decided to avoid startup times when the activity is decided as well as free up processing power that might be needed if the user is just having a general conversation with the LLM. 

### Day 3:
Today I focused on being able to stream audio in from the user to prevent overall system down time, as well as putting the Pose detection on a thread that could be started and stopped when a user wishes to perform an exercise or stop that exercise. Firstly to start this, I had to change the chat loop that I had originally been working with to implement a callback so that I could continually stream in audio while working with that audio at the sime time for the Vosk ASR. Once this was working as intended I had implemented keyword searches so that as the user is talking live it will detect keywords for stopping or pausing as if a user is in the middle of an exercise and need to stop right away we didn't want the user to have to wait on system down time to process that. Additoinally, I created a dictionary that can be used to store information on the different exercises or therapy activites that would be offered. This was to help keep consistency and formatting to then be able to parse the users audio input for keywords relating to the different activities for a more real-time experience. This dictionary holds the name of the exercies, the muscles it works on/with, as well as a short descrption of how to perform the motion. Once I had detection for what was considered an exercise I then wanted to parse for a numerical value spoken by the user to determine weather the user was seeking explaination or actually wanted to perform an exercise. For example, a user saying "What is a bicep curl" would find the keyword "bicep curl" and the explaination fo the motion would be played, wheras if the user said "I want to do five bicep curls" the program would then process that as wanting to start the bicep exercise for 5 repetitions. Finally, now that I had determined what would be the start condition for an exercise, I know where to start and stop the thread for pose detection. Currently we have parameters hard coded into the pose thread, and we intend to use the shared state data structure that we are using for communication between the chatbot and pose detection to alter the exercise that is being tracked in the pose file for the ability to perform multiple different exercises in a single execution of the program.

### Day 4:
Today we focused on the communicatoin between the pose detection and the chatbot files, and what one of the files may need communicated from the other. This was to have a plan to approach our exercise setup and completion to make sure that the user was performing the exercise correctly as well as maintaining a live feedback loop that way the user can be corrected as they complete the exercise. This would all be happening while the user is still able to communicate with the LLM as the calculations are done elsewhere and the chatbot file deals with the flags for when the system should be talking to the user to correct them as well as taking their input in some cases. For example we added a check that if the user goes a certain amount of repetitions not reaching the full range of motion of the exercise that the system will then prompt them if they would like to make the range of motion easier. The user can then respond with yes or no depending on their prefrence and the pose detection will then calulate a rep at an easier or the same range of motion. Additionally if the user is determined to have bad form based off of the angles calculated from their body, the shared data will update a list of "bad forms" so that I can iterate through the areas of bad form correcting the user as I iterate through the different corrections that have an adjustment dialog in their dicitonary. Once we were both finished implementing the getters and setters that we needed for communication we then went to test and debug anything that we could as well as started planning for further improvements that could be made using the shared state. 

### Day 5:
Today I focused more on wuality improvements and small bug fixes until we start to implement more exercises into the enviornment. Specifically I focused on changing how the user pauses their exercise when in the middle of an exersise as well as stopping an exercise opposed to stopping the whole program. Firstly I implemented new logic for waiting, such that the program would only "pause" if the user was in the middle of an exercise becasue otherwise the user wouldn't neccesarily have to pause the machine as the machine wouldn't be detecting anything except audio from the user adn weather they wanted to prompt the AI or start a new exercise. Additionally now when the user pauses, the program doesn't pause for a set time but instead until the user determines weather they wish to continue the same exercise, start a new exercise, or stop the exercise entierly without ending the entire program/the conversational AI as well. This allows for a much more realistic conversation between the system and the user and a more seemless transition for pausing as before the entire program was just sleeping for 5-10 seconds. Additionally, now when the user is in an exercise and they want to stop right away or just say the word "stop" the program will stop the exercise and not stop the entire program unless they indicate that they want a full system stop. ALlowing for the user to have more control over their enviornment when working with the device. Finally I wanted to start researching more specific exercises for the communities that we intend to be working with. I was able to find many exercises for post-stroke patients such as seated leg lifts, which work the muscles of the body without putting too much strain on the body for the user. I wish to continue growing this list of specialized exercises as well as the information surrounding the exercise such as the areas of the body it works, its intended goal for the user, and a brief exlaination of how to perform the exercise, while also keeping track of how to determine if the user is using good or bad form for these new exercises. 

## Week 4:

### Day 1:
This week one of our main goals is to cut down on some system latency and contintue to optimize our real-time pipeline. Additionally, we want to implement more gamefied features for the exericses that we have working with our pose detection to have more presentable features other than the audio chatbot. Specifically today I focused on making sure all the changes we've been making locally are consistent and working on the raspberry pi, as it is a different device with lower capabilities than our personal laptops. Some things worth note were that the TTS model (Kokoro 82M) was seeing about a double in processing time, meaning things that would take say five second on my local machine was now taking ten seconds on the raspberry pi. Therefore I spent a good bit in the beginning of the day trying to improve my implementation of threading to hasten the processing of the model. Additionally a big problem that I found was when finishing an exercise on the raspberry pi, when you go to start another exercise the camera doesnt seem to display or work. I have spent the majority of today trying to debug what could be happening but it has been extremely challenging to diagnose. Firstly this is because we have to use different packages for the camera on the raspberry pi than on our local machines. On local we can simply use opencv wheras on the raspberry pi we must use picam to work with the wired camera, though we also have the option of opencv for the webcam that is using a USB port. Though lcoally running using opencv works, using either opencv or picam on the raspberry pi both run into the same problem which seems to be that the thread or camera objects aren't being closed or ended correctly, therefore when trying to reopen the camera there are issues. I plan on revisiting the threading that I had done for the pose detection, where it only starts and stops when the user specifies, and maybe attempt different threading approaches as my approuch could just be misguided. Though I also want to look into the picam and opencv packages to try and determine the best way to end the use of their more effectively. Finally in the beginning of the day I had compiled a list a new exercises that could be appeneded to the current ones that we have, these ones more focussing on post stroke recovery. This would hopefully give us more exercises to work with on testing the general functionality of our enviornment as well.

### Day 2:
Today I worked on fixing the problem that is encountered when you go to start a new exercise on the Raspberry Pi. When I first approached the problem today I had the belief that it simply had to do with the camera and the way that we were accessing it and cleaning up the access when closing the thread. After running through multiple attempts and adding print flags throughout the file I was able to find the exact line that the program was getting stuck on and it actually had nothing to do with the camera being accessed but instead when we were trying to display the information that we were recieving from the camera. The specific line of code was in our yolo_threaded file, cv2.imshow(), which is the line for displaying the frame processed from the camera/model and outputting it into a window for the user to view. The worst part about the bug is that when encountered there have been no exceptions passed or logs sent to the machine so I have been trying to almost blindly fix the problem. Though after using resources like stack overflow and general informational websites I believe the error comes from the fact that the GUI is being accessed/altered by somewhere that isnt a "Main thread" and thus just freezes/fails to complete. Although when the exercise is first ran as a thread the disply is working, I believe this problem comes from the fact that you are essentially stopping and starting the thread, and once the thread is ran to completion the first time the Raspberry Pi then doesnt know how to process the GUI interactions once the thread is started a second time. To fix this problem, I am planning implementing a display function in the streaming file that can be used to display the informaiton that is being calculated from the yolo_threaded file. Using shared state we would be able to communicate the frames that are being processed and then display them in only the main file by having the display loop play at the same points as the yolo thread thus they would always be running together, and display loop would make a call to stop the pose thread when the exercise was complete, making sure that there are no extra threads running in the background that werent terminated. As to the reason that the program is working locally but not on the Raspberry Pi, this could simply becasue it doesn't have the GUI capabilities that our local devices have, which would be better prepared to handle threaded GUI interactions. Additional improvements that I found would improve the user experience would be adding a parse to wait for the user to say "start" to start their exercise if they have to get to a certain position or if they are indicated to move a certain distance from the camera, thus to avoid problems of the user being corrected for form before they even mean to start the exercise. Additionally, finding a different way to iterate through the form corrections for the user, as cuttenly when iterating through the list of bad forms, the TTS will take its time processing each individully, and during this time there can be odd gaps between the different forms being corrected, while also not picking up user speech. This leads to a lot of system down time where the user essentially cannot do anything if they do not fix their form, which in some cases may be impossible for the user if they simply struggle with the exercise or didn't mean to select that one. 

### Day 3:
Today I continued with getting the Raspberry Pi to display the camera information from the user on more than just the first exercise. Starting off the day I wanted to try and use the Shared State file that we have been usin for thread safe transfer of information to see if we are able to transfer the frames from the pose detection over the shared state to a seperate file that way the display wouldnt be gettting processed on a thread as that was the main problem that we were running into. Eventaully today I was able to find the specific errors which werent printing the last two days, them being QT object and platform errors, specifically stating theat what we were trying to do on a thread just isnt possible on this device. Once this was confirmed, we decided to try and implement all of the imge displaying in the main funciton of the stream_test file, as since it was running the chatbot loop as its own funciton which had a callback, we could run that on a thread while updating the display in the main. Therefore we essentially transitioned our project to being completely on threads, being that the conversational AI is on a thread, and then the YOLO pose thread is started from the Conversational thread when the user inputs that they want to start an exercise. Technically the only thing happening in the main loop is starting the thread and then chekcing for frame updates and if there are to display them and if there arent, then the user isnt performing an exercise, and thus to destroy the windows for future exercises. Finally through debugging this problem and just overall use on the program, I have decided to change the way that we are parsing prompts from the user. To avoid confusion, I am making sure that the "arise" keyword is the first thing the user states in the prompt due to the microphone starting to pickup audio output from the system more often. Though when the user is in a conversation with the system, for example they pause their exercise and are asked weather they would like to continue, start new, or stop, the user doesnt have to use the prompt keyword as it is awaiting their response already before continuing to something else. Therefore someone can say "arise wait" and then "stop" if they simply wished to stop their exercise. Finally, I have realized that there is desperate need to change the way that we are performing the pose corrections, as currently when the TTS model is iterating and speaking the form corrections out, the user cannot input anything to the system, so in the chance the user made a mistake and picked the wrong exercise they could be stuck listening to bad form corrections before they are able to stop the exercise like they wished to from the start, additionally I want to implement a "start" keyword to start the exercise only when the user says to so that they can have time to setup if needed for certain exercises. 

### Day 4:
Today we found that the error of the second exercise problem was solved but in solving that process, when the user went to pause their exercise adn then stop the exercise the system would not be able to close the window and thus crash. To get around this, I used the current_exercise variable which holds the name of the current exercise, and if there is no exercise the value is set to None. Therefore I can check in the main loop that if current exercise is none I should then destroy all the windows, but only if a window is open. Thus I added a flag check in the main that is started as false called window flag, where originally when the user starts they arent in an exercise and instead tell the system when they wish to complete an exercise and what, and once they do and the window appears the window flag is set to true, therefore when the exercise is completed or stopped and the window needs to be closed, the main funciton will know that both there is no exercise in progress and that there is also a window open to be closed. Giving us a seemless start and stop on both the raspberrhy pi and our local devices. After getting these majow bug fixes done, I decided to start improving the way that bad form is corrected for the user. Before the bad form list would have a list of the users bad form that is then iterated through to tell the user to correct it, but this process would be blocking and tim consuming as well as in most cases when a user fixed one area of form they would also fix most other areas of form. Therefore after the system alerts the user of their bad form, instead of iterating through every case of bad form, the shared state is accessed to see if there is any update in the bad form list. If the bad form list is changed to having no bad form, then the systme will stop trying to correct the user, wheras if the user fixed their form in some areas but not others only the areas of bad form that are currently observable are refrenced. Finally I added a keyword check for the user saying that they are ready for their exercise. This is through the use of a flag, that instead of starting right away when the user wants to start an exercise, this flag is enabled and then makes the system wait for the words of "ready" or "start" to actually start the pose detection and open the window for the user. 

### Day 5:
Today I put more research into reducing the TTS latency as that seems to be where the bottleneck currently lies in the program. This is because when the sytem is performing TTS sometimes the latency will be really good and other times it will exponentially increase for no apparent reason. To try and get around this inconcistency I intend to implement smaller models that hopefully wouldnt run into this latency issue as it will need less cpu usage to produce similar audio. To do this I want to test different quantized versions of the kokoro model to try and have as human sounding an assistant as possible, though if none of the available quantized models are able to perform up to standard then I wish to try piper TTS. Piper TTS was made and optimized for the raspberry pi, it is very compact and quick but has an extremely robotic sounding voice, and I feel should be only implemented if there are consistent problems with the kokoro, more human sounding, model. I was able to find a setup for a int8 quantized version of Kokoro which I have been attempting to integrate into testing with our original chatbot file, which I can then follow the same producer of the int8 version as they have all the other available quantized versions as well such as fp32. The main goal is to have the whole system real-time, so if it is more realistic to use the piper TTS in our scenario due to running other processes other than TTS at the same time, unless we are able to speed up processing by seperating the computer vision and NLP through the use of the hailo board on the new Pi. Additionally, there needed to be some minor fixes to the yolo threaded file, as due to there being a destory all windows call in the threaded file, sometimes that line would be called before the main threads call and thus crash the window as a thread other than main was trying to access the GUI. 

## Week 5:

### Day 1:
Today I focused on implementing the different quantized versions of Kokoro into our conversational chatbot system to make sure that everything would be able to work cohesively with all the packages on the raspberry pi. While doing this I encountered a problem that numpy has differing version in both the Kokoro-82m and the kokoro-onnx, where the base 82m model needs a version of numpy below 2.0 and the kokoro-onnx model needs numpy above 2.0. This isnt too big of a problem as since they are both a TTS model, when I am runnign the one I just have to comment the lines out that pertain to the other model while making sure that I switch between the different versions of numpy. Additionally to get around this I have thought of the idea of saving numpy to different names for the different versions such as numpy-1 being numpy 1.26.4 which is the version we were using for the base Kokoro-82m and numpy-2 being version 2.3.0 which is the version being used for the quantized models. Additionally today I wanted to improve the keyword parsing even more as  while working with the OMC interviewers and trying to demo the project for them, I realized just how much background noise the microphone and the vosk model were picking up,  leading to the demonstration being a little awkward as I had to stop and wait for the buffer to clear if any words were spoken before trying to prompt the system as to prompt the system you needed the "arise" keyword said as the very first word in your buffer. This makes sense theoretically as it would typically be how you start your conversation with the system but if theres background noise or say the speech to text model prosesses "arise" as something else the system would then have to wait a little with no noise to clear the scentence buffer. Now when the user is making noise, no matter what it will look for the word 'arise' or any other prompt keywords we might want to add and then processes everything in the scentence buffer after the keyword. This has allowed for a much more smooth conversation between the user and the system. Through this experience I wish to have more users unfamiliar with our project help stress test the conversational side of the model to make sure that there are no intricacies that are missed that might cause problems. Things that I am planning on working on tomorrow are making producer and consumer threads using the onnx kokoro models and also finding a way to break texts into chunks for those producers and consumers to reduce overall latency of the conversation.

### Day 2:
Today I wanted to focus on optimizing the quantized version of Kokoro to implement a consumer and producer thread to have audio playing while the process is generating the audio for the next chunk of words. To do this I first had the audio that is going to be spoken to the user broken up by scentences using ".","?", and "!" to have a natural flow for the chunks. When this was first iplemented du to sentences havign the ability to be different lengths, a runon sentence could lead to there being awkard delay between chunks being played, so i added a second check that makes sure the sentence is 20 words or shorter and if now to split the sentence into half. Once implemented the audio was able to play fully without any awkward pauses but was then startign the scentences almost immediatly, therefore ending a word and almost immediatly starting the next when between chunks, so I added a small sleep which depends on the length of the text that it had just processed to rest for a little before starting the next word. For example a sentence that hits that exact 20 word mark would have the most delay between starting the next sentence and a sentence with say 10 or 5 words would have a shorter delay. Although this is very slight, I feel it really helped with the quality of the sound and makes it imitate a speaker taking a breath between continuing to speak. Once I teste the threading, I then implemented it into the current chatbot logic we had and it decreased the latency by a good margin while also staying more consistent and smooth sounding. Finally to finish off the day I wanted to start making testing files to get the overall latency of eeach peice of the project, that is STT latenct, LLM prompting latency, and TTS latency which I was already testing and had a test file for. I have been running and making them as consistent with the projects current pipeline as possible, as some areas had to be changed for testing. For instance, the STT in out project uses a callback to continue to take input from the user, but in the test file where I want to get the average latency over a number of repetitions I would need an audio that was consistent and unchanging for latenties that arent affected by any external factor. Additionally with the LLM, when prompting there are multiple different responses that could be given to the same exact prompt, therefore adding a layer of unpredictability to the test. In some of the tests i've done so far just chekcin if the test file was working the Processing time jumped from 14 seconds to 4 and when checking the prompts they output they were vastly different in quality of response, but they had been prompted the same exact way as eachother. Overall my goal for the rest of this week is to have as consistent testing files as possible with our project, and to optomize and reduce any latencies anywhere in the pipeline that may be considered a bottleneck. 
