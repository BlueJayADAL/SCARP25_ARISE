# SCARP25_ARISE - Weekly Journals

## Table of Contents
- [Week 1](#Week_1)

## Week 1:
### Day 1:
Focused on getting a deeper general understanding for the project and its intended direction as well as a better feel for the features that we want implemented for this summers research. Additionally division of work to make sure that there as an equal distribution for each of our ecxpereince levels and comfortability with the different tasks at hand. Aaron has a better understanding of hardware than me and I plan on learning from him these first couple of days to better understand how to work with hardware. The main focus for today was to get the Jetson Nano devices to run and connect to a monitor for display. To do this we had to troubleshoot power issues as well as make sure that the pin outs of the SDK are working as intended with the diffferent attachments such as the fan for cooling the GPU. Additionally once able to diplay the information from the devices onto a monitor we were then able to check the software capabilities of the SDK's as we have to ensure the drivers and softwares installed are up to date and compatible with the models that we intend on implementing. For instance the Jetson Nano device from seed studio was running on ubuntu 18 which is outdated for most software packages. 

### Day 2: 
Starting day 2 the focus was updating Ubuntu as well as increasing the amount of swap space, to prevent memory errors when running out of RAM. While attempting to update Ubuntu, I had to connect the Jetson Nano to the internet and through this process realied that the decives had to be registered by the campus to be used on the internet. Therefore even with a wired connection we werent able to access the internet to get the necessary files needed for updating Ubuntu. Due to this, I started a focus on how we are going to incorperate text to speech into our system. Today I did research as well as implemented test files using Vosk, a text to speech model based on neural networks, and was able to have test files for both microhpone input as well as from an audio file such as MP4. Specifically Vosk models work with .wav files but the majority of examples that I found were converting MP4s to wav and then predicting on the data to determine what is being said. The microphone input is what is more suited towards our project, as it is offline speech to text deciphering, though using the microphone makes it hard to test edge cases like accents and background noise. Therefore I spent a good amount of time also just looking for audio files that either had background noise, making it harder to understand the words, or someone with a thick accent speaking. Doing this to then go back and test on the audio files to see the threshold at which background noise becomes too much, or where an accent is too thick, as well as different approaches to mitigate these.
  