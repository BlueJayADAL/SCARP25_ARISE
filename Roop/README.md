# SCARP25_ARISE - Weekly Journals

## Table of Contents
- [Week 1](#Week-1)
- [Week 2](#Week-2)


## Week 1:
### Day 1:
Focused on getting a deeper general understanding for the project and its intended direction as well as a better feel for the features that we want implemented for this summers research. Additionally division of work to make sure that there as an equal distribution for each of our ecxpereince levels and comfortability with the different tasks at hand. Aaron has a better understanding of hardware than me and I plan on learning from him these first couple of days to better understand how to work with hardware. The main focus for today was to get the Jetson Nano devices to run and connect to a monitor for display. To do this we had to troubleshoot power issues as well as make sure that the pin outs of the SDK are working as intended with the diffferent attachments such as the fan for cooling the GPU. Additionally once able to diplay the information from the devices onto a monitor we were then able to check the software capabilities of the SDK's as we have to ensure the drivers and softwares installed are up to date and compatible with the models that we intend on implementing. For instance the Jetson Nano device from seed studio was running on ubuntu 18 which is outdated for most software packages. 

### Day 2: 
Starting day 2 the focus was updating Ubuntu as well as increasing the amount of swap space, to prevent memory errors when running out of RAM. While attempting to update Ubuntu, I had to connect the Jetson Nano to the internet and through this process realied that the decives had to be registered by the campus to be used on the internet. Therefore even with a wired connection we werent able to access the internet to get the necessary files needed for updating Ubuntu. Due to this, I started a focus on how we are going to incorperate text to speech into our system. Today I did research as well as implemented test files using Vosk, a text to speech model based on neural networks, and was able to have test files for both microhpone input as well as from an audio file such as MP4. Specifically Vosk models work with .wav files but the majority of examples that I found were converting MP4s to wav and then predicting on the data to determine what is being said. The microphone input is what is more suited towards our project, as it is offline speech to text deciphering, though using the microphone makes it hard to test edge cases like accents and background noise. Therefore I spent a good amount of time also just looking for audio files that either had background noise, making it harder to understand the words, or someone with a thick accent speaking. Doing this to then go back and test on the audio files to see the threshold at which background noise becomes too much, or where an accent is too thick, as well as different approaches to mitigate these.

### Day 3:
Today I wanted to take my focus back onto the Jetson Nano device as Aaron was able to connect his device to the wifi early on in the day. While Aaron was working on connecting his device to the internet, I worked on applying different english models for Vosk that we could use for Speech to text recognition. The intention of using all the latest available english models was to find which model worked the best for audio that may have an accent or background noise during its processing. This is due to the fact that many people have varrying accents as well as the background noise in different enviornments vary, and in the instance that someone has a thicker accent or lives in a busier, louder area, we wouldnt want our product being unintentionally discriminitory for those populations. I found that the larger the model the more time it would take to startup the model, as there is more information that needs to be loaded, though the larger models tended to struggle with audio that had an accent present. This could be due to the fact that Vosk models are Nueral Networks which have a tendency to overfit to their training data. Therefore having a larger model of a Nueral Network, which already has a tendency to overfit, could lead to even more overfitting errors. In the case of the audio files I was using for testing the larger models tended to struggle with larger words due to accents, and tried breaking them down into smaller easeir to determine words, that although may sound the same or similar to what was intended just gramatically wouldn't make sense. When it came to testing background noise though, there seemed to be an inverted bell curve for what could be called accuracy. For the audio with background noise I more so determined the success by how well the models were able to pickup any words that were said over a loud background. In this case the smaller models and the larger models were able to pickup on generally more words while the models in the middle were less likely to pickup words being said. Overall leading to my determination that we should use the Vosk's english graph model for implementation as it has a balance between accruacy, detection, and startup speed that would be realistic for our project's intentions. Finally after working on model selection, I attempted to connect my Jetson Nano to the internet and seemed to be able to. Once one the device I tried installing the software update for Ubuntu 20.04 though when the system rebooted the GUI was no longer working. I spent the rest fo the time trying to fix te GUI and may have to revert the software update to fix the packages that may have been corrupted.

### Day 4: 
Today I wanted to focus on trying to fix any packages or problems that had occured from attempting to update my Ubuntu to 20.04. Once the update was finished and the system rebooted there was an error where the system wasn't able to access address 0x50 which is thd address for the information stored in the EEPROM, to which this wouldnt allow anything to process. Once this was found out I had determined that it was necessary to reflash the device in order to revert any corrupted packages or files that were no longer compatible. Additionally I wanted to do extra research into speech-to-text models, and attempt to implement NVIDIA's Jarvis ASR for speech comprehension. This is due to the fact that we are using Jetson devices built with NVIDIA's parts, therefore the models may be more optomized for the devices that we are intending to use. I also wanted to research the implementation for a Deep Seek model for the use of conversational AI. This is because if we are processing the words spoken into text, we then have to have a model process that text to understand the text, and to do this we plan on using deep seek which is an open sourced generative AI. Understanding this model and how it should handle input can help with the decision between the speech-to-text models as one may be better suited for the processing of text. Overall this is to keep the flow of infrmation as quick and seemless as possible to reduce the amount of latency that would be in this offline system. 

### Day 5:
Starting off the day I wanted to pu more research into the possibility of using Jarvis/Riva from NVIDIA's documentation. Through doing this I was able to understand that the higher demand for storage and system requirements for Jarvis are necessary due to the fact that Jarvis has inherent NLP and TTS models on top of the base ASR model. This means that not only does the model diest human speech but it also can process it and spit out an output which is also audio. This is pretty much identical to the conversational AI concept that we have been planning on implementing though wich much steeper requirements. Therefore I started myresearch into a full pipeline for using Vosk to process words to text, and then a Llama LLM to process the text that is generated and then a TTS model to then output the LLM's response to the user. For testing I am using the smallest possible models to avoid any problems that could be encountered through system requirements as well as time taken downloading the models locally. While working on implementing all three of the models together I kept running into errors resolving packages and dependencies due to the fact that a lot of the packags needed werent already on my system, and as I tried to install them they would need otherpackages that I also do not have. Additionally the final problem I ran into was that I needed to change my python enviornment as Llama models cannot be loaded on Python if its using Anaconda to run python, therefore I had to change the way in which my system uses python as well as re-install all the packages that I already needed for my python before. I have yet to finish the implementation due to running into errors though will breakdown the pipeline to test peice by peice next time.

## Week 2:
### Day 1:
Today I focused on implementing a complete virtual enviornment for the conversational AI, as it was running into versioning problems the other day. Once I had done more reasearch, and reviwed posts on Stack Overflow I found that the most optimal version of Python was 3.11 for both the TTS and Llama-cpp for loading LLM models. Once I had switched my verions of python as well as installed the necessary packages in the virtual Enviornment I was able to run through laoding all the mdoels with no issues. Once the models began to load I was noticing some peculiar tendencies as well as outputs from the program. In my first implementation, the audio was output through the Media Player application on my windows device. This wouldnt be a good practice to have for testing as we plan on implementing these models into a SDK which is not a Windows machine and thus I installed the Pydub and simple audio packages to handle the outputting of audio from the device as to not need to use any external apps outside of the python code. Additionally sometimes when the model would output its text,  the text would have formatting assigned to the output, which would lead to a different display in the models normal API though in our case was only returning weird ouputs. For instance I prompted the model to solve a math equation and once it was done with the formula and stpes tried to box the answer using a format of "boxed12345", Im assuming trying to put a box around the value of "12345". To handle this so that the TTS model doesn't crash or give an output that isn't gramatically correct or makes sense, I added an additional function that uses regular expression to parse the text form the output of the LLM to remove any irregular characters that wouldnt make sense in human speech. In preparations to try and compare different LLM's and TTS models, I added timestamps for the start and end of each model when it is prompted or given an input, to help determine which model is the most effecient. Although the speed at whcih the information is processed is not the only metric, as reasonability and accuracy are also the main focus of the models to which they also have to be compared.
  