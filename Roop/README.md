# SCARP25_ARISE - Weekly Journals

## Table of Contents
- [Week 1](#Week-1)
- [Week 2](#Week-2)
- [Week 3](#Week-3)


## Week 1:
### Day 1:
Focused on getting a deeper general understanding for the project and its intended direction as well as a better feel for the features that we want implemented for this summers research. Additionally division of work to make sure that there as an equal distribution for each of our ecxpereince levels and comfortability with the different tasks at hand. Aaron has a better understanding of hardware than me and I plan on learning from him these first couple of days to better understand how to work with hardware. The main focus for today was to get the Jetson Nano devices to run and connect to a monitor for display. To do this we had to troubleshoot power issues as well as make sure that the pin outs of the SDK are working as intended with the diffferent attachments such as the fan for cooling the GPU. Additionally once able to diplay the information from the devices onto a monitor we were then able to check the software capabilities of the SDK's as we have to ensure the drivers and softwares installed are up to date and compatible with the models that we intend on implementing. For instance the Jetson Nano device from seed studio was running on ubuntu 18 which is outdated for most software packages. 

### Day 2: 
Starting day 2 the focus was updating Ubuntu as well as increasing the amount of swap space, to prevent memory errors when running out of RAM. While attempting to update Ubuntu, I had to connect the Jetson Nano to the internet and through this process realied that the decives had to be registered by the campus to be used on the internet. Therefore even with a wired connection we werent able to access the internet to get the necessary files needed for updating Ubuntu. Due to this, I started a focus on how we are going to incorperate text to speech into our system. Today I did research as well as implemented test files using Vosk, a text to speech model based on neural networks, and was able to have test files for both microhpone input as well as from an audio file such as MP4. Specifically Vosk models work with .wav files but the majority of examples that I found were converting MP4s to wav and then predicting on the data to determine what is being said. The microphone input is what is more suited towards our project, as it is offline speech to text deciphering, though using the microphone makes it hard to test edge cases like accents and background noise. Therefore I spent a good amount of time also just looking for audio files that either had background noise, making it harder to understand the words, or someone with a thick accent speaking. Doing this to then go back and test on the audio files to see the threshold at which background noise becomes too much, or where an accent is too thick, as well as different approaches to mitigate these.

### Day 3:
Today I wanted to take my focus back onto the Jetson Nano device as Aaron was able to connect his device to the wifi early on in the day. While Aaron was working on connecting his device to the internet, I worked on applying different english models for Vosk that we could use for Speech to text recognition. The intention of using all the latest available english models was to find which model worked the best for audio that may have an accent or background noise during its processing. This is due to the fact that many people have varrying accents as well as the background noise in different enviornments vary, and in the instance that someone has a thicker accent or lives in a busier, louder area, we wouldnt want our product being unintentionally discriminitory for those populations. I found that the larger the model the more time it would take to startup the model, as there is more information that needs to be loaded, though the larger models tended to struggle with audio that had an accent present. This could be due to the fact that Vosk models are Nueral Networks which have a tendency to overfit to their training data. Therefore having a larger model of a Nueral Network, which already has a tendency to overfit, could lead to even more overfitting errors. In the case of the audio files I was using for testing the larger models tended to struggle with larger words due to accents, and tried breaking them down into smaller easeir to determine words, that although may sound the same or similar to what was intended just gramatically wouldn't make sense. When it came to testing background noise though, there seemed to be an inverted bell curve for what could be called accuracy. For the audio with background noise I more so determined the success by how well the models were able to pickup any words that were said over a loud background. In this case the smaller models and the larger models were able to pickup on generally more words while the models in the middle were less likely to pickup words being said. Overall leading to my determination that we should use the Vosk's english graph model for implementation as it has a balance between accruacy, detection, and startup speed that would be realistic for our project's intentions. Finally after working on model selection, I attempted to connect my Jetson Nano to the internet and seemed to be able to. Once one the device I tried installing the software update for Ubuntu 20.04 though when the system rebooted the GUI was no longer working. I spent the rest fo the time trying to fix te GUI and may have to revert the software update to fix the packages that may have been corrupted.

### Day 4: 
Today I wanted to focus on trying to fix any packages or problems that had occured from attempting to update my Ubuntu to 20.04. Once the update was finished and the system rebooted there was an error where the system wasn't able to access address 0x50 which is thd address for the information stored in the EEPROM, to which this wouldnt allow anything to process. Once this was found out I had determined that it was necessary to reflash the device in order to revert any corrupted packages or files that were no longer compatible. Additionally I wanted to do extra research into speech-to-text models, and attempt to implement NVIDIA's Jarvis ASR for speech comprehension. This is due to the fact that we are using Jetson devices built with NVIDIA's parts, therefore the models may be more optomized for the devices that we are intending to use. I also wanted to research the implementation for a Deep Seek model for the use of conversational AI. This is because if we are processing the words spoken into text, we then have to have a model process that text to understand the text, and to do this we plan on using deep seek which is an open sourced generative AI. Understanding this model and how it should handle input can help with the decision between the speech-to-text models as one may be better suited for the processing of text. Overall this is to keep the flow of infrmation as quick and seemless as possible to reduce the amount of latency that would be in this offline system. 

### Day 5:
Starting off the day I wanted to pu more research into the possibility of using Jarvis/Riva from NVIDIA's documentation. Through doing this I was able to understand that the higher demand for storage and system requirements for Jarvis are necessary due to the fact that Jarvis has inherent NLP and TTS models on top of the base ASR model. This means that not only does the model diest human speech but it also can process it and spit out an output which is also audio. This is pretty much identical to the conversational AI concept that we have been planning on implementing though wich much steeper requirements. Therefore I started myresearch into a full pipeline for using Vosk to process words to text, and then a Llama LLM to process the text that is generated and then a TTS model to then output the LLM's response to the user. For testing I am using the smallest possible models to avoid any problems that could be encountered through system requirements as well as time taken downloading the models locally. While working on implementing all three of the models together I kept running into errors resolving packages and dependencies due to the fact that a lot of the packags needed werent already on my system, and as I tried to install them they would need otherpackages that I also do not have. Additionally the final problem I ran into was that I needed to change my python enviornment as Llama models cannot be loaded on Python if its using Anaconda to run python, therefore I had to change the way in which my system uses python as well as re-install all the packages that I already needed for my python before. I have yet to finish the implementation due to running into errors though will breakdown the pipeline to test peice by peice next time.

## Week 2:
### Day 1:
Today I focused on implementing a complete virtual enviornment for the conversational AI, as it was running into versioning problems the other day. Once I had done more reasearch, and reviwed posts on Stack Overflow I found that the most optimal version of Python was 3.11 for both the TTS and Llama-cpp for loading LLM models. Once I had switched my verions of python as well as installed the necessary packages in the virtual Enviornment I was able to run through laoding all the mdoels with no issues. Once the models began to load I was noticing some peculiar tendencies as well as outputs from the program. In my first implementation, the audio was output through the Media Player application on my windows device. This wouldnt be a good practice to have for testing as we plan on implementing these models into a SDK which is not a Windows machine and thus I installed the Pydub and simple audio packages to handle the outputting of audio from the device as to not need to use any external apps outside of the python code. Additionally sometimes when the model would output its text,  the text would have formatting assigned to the output, which would lead to a different display in the models normal API though in our case was only returning weird ouputs. For instance I prompted the model to solve a math equation and once it was done with the formula and stpes tried to box the answer using a format of "boxed12345", Im assuming trying to put a box around the value of "12345". To handle this so that the TTS model doesn't crash or give an output that isn't gramatically correct or makes sense, I added an additional function that uses regular expression to parse the text form the output of the LLM to remove any irregular characters that wouldnt make sense in human speech. In preparations to try and compare different LLM's and TTS models, I added timestamps for the start and end of each model when it is prompted or given an input, to help determine which model is the most effecient. Although the speed at whcih the information is processed is not the only metric, as reasonability and accuracy are also the main focus of the models to which they also have to be compared.

### Day 2:
For the focus of today I wanted to work on getting a better understanding on the dependencies and the viability of different TTS models to improve personability and speed. Currently the TTS model that I was using was more so jsut the only one that I was able to get working, though I have been able to find more documentation and resourses like stackademic and some of their command line commands that allow me to get a better understanding of what came with the package I have installed. My focus was to implement a model that could both emulate human speech patterns, making the spoken audio to sound more human like, while also reducing the amount of time taken to also process the text. For the personability, I attempted implementing Xtts_v2 which is Coqui's latest model for TTS though even after fixing issues with install dependencies with torch, there were deeper libraries within the Xtts model which were causing errors that I wasn't able to resolve or understand, leading to my choice in testing the your_tts model. This model is similar to previous TTS models although allowing you to use a wav file for the model to breakdown and use to recreate the audio for the text that you are parsing. Although this model is more complex than the base model I was using, it was still processing the text much faster. Although the model, speedy-speech, was by far the quickest model to process any text that I had given it. This model focuses on getting as close to real-time processing as possible by processing the data in a entirely different way than the other models. Firstly, all the models break down the text input into multiple scentences, processesing them one at a time as to not go over character limits for the TTS model, though speedy speech takes this a step further by convertign the text to the Phonetic Alphabet. Transforming text to this allows for a more robust understanding of the noises that are supposed to be made from the words that are written. Therefore allowing for the model to be more generalized as specifically in the english language we have many words that sound exactly the same but are spelled differently and this model only cares about the sound its making not necessarily the grammer behind it. Finally I've also started to put more research into the different types of LLM's that could be implemented for the project to help reduce the amount of time producing a output for a prompt. As there are different types of models focused on more niche aspects of conversation, some models are trained specifcially for chatbot purposes while others are more of a coding assisant and others could be intermediate models, have little training done, or base models, no fine-tuning. Overall I know that the number of parameters, usually denoted by a b(billions), needs to be reducd as that is the main factor in time complexity for machine learning models as they have to run through all the parameters for the data that they are processing. 

### Day 3:
The focus of today was shifting the use of TTS models form the TTS package in python from Coqui to the Kokoro-82M model, which is a quicker, more humanoid model. Through the article that Dr.Li had given me, I was able to recognize that since Kokoro had already been running locally for the person who had written the article, that it would be optimal to implement for our purposes as we already know it would be compatible with our hardware. Additionally, when testing the model, it was processing as fast as the quickest TTS models from the TTS package while also retaining the more humanoid speech. Implementing Kokoro was a little odd as they had multiple available versions of the package such as Kokoro and Kokoro-onnx. I found little documentation that was actually working for the implementation of Kokoro-onnox leading me to the Hugging Face repository for the base Kokoro package. Following the focumentation I was able to use a test model to actually hear what the model sounded like, while also downloading all necessary model files to my system. Once all the files were downloaded I was then able to locally run the TTS model, allowing for implementation into my Chatbot file. Firstly I noticed that it was processing the text the same was as the Speedy-Speech model from Coqui, by transforming English text to the Phonetic spelling of the words for easier determination of the sound, while also breaking the text prompt into more digestable chunks. Although a key difference I noticed was that the Kokoro model was trying to output the audio is it processed the text chunk by chunk, leading to the overall output to have odd breaks between sentences at times. Though this may seem wrong, I have the belief that implementing threads to handle this process could allow for the model to perform translation to audio even quicker, though I do not know the feasability of this in the context of use on our raspberry pi. I plan on testing more things out with threads in the coming days to decrease the overall response time of the chatbot for as close to real-time processing as possible. 

### Day 4:
Today I focused on implementing the use of threads to process the text that would be coming from the LLM model. This is becasue when I was using the Kokoro TTS model, I realized that it was breaking down the large paragraphs into digestable chunks for the model to handle easier, therefore one woule be able to process these chunks in parallel to further decrease the time it takes processing the data. To do this I implemented a Producer and a Consumer where the Producer in the speak function, produces the audio files that will be played by the device, where the consumer makes sure that the audio files are in the right order as well as all processed as to not ave any delays between the files as well as to aviod the text getting played out of order and not making any gramatical sense. Additionally I spent some time today looking into the documentation of LLama-cpp-python and the functions that I was using from the package to better understand the parameters. When loading the model into the project, theres a parameter allowing me to specify the number of threads that I may want for the model. Meaning that I could also implement threads potentially into the LLM to increase the processing speed. Additionally when generating a response there are even more parameters which could be useful for our purposes. Such as the streaming parameter, where if True will produce the tokens from the model as they are generated. Theoretically, I would wish to implement streaming so that the model can produce the tokens and that the TTS model can process those tokens into audio as they are produced from the model. Essentially leaving no down time between the text being produced and then the audio for that text playing. To which the audio can continue playing as more and more text is generated and processed. Currently I am working on implementing a working version of this, but the audio keeps coming out choppy/ with odd delays between different tokens. 

## Week 3:
### Day 1:
This weeks focus is on getting both pose recognition and the conversational chat bot running on a script at the same time on the raspberry pi. Our goal is going to be havving range of motion detection and tracking for the arm of a user, as well as having a breakdown of the exercise after by the convesational AI. To do this, today we focused on making a virtual enviornment on the raspberry pi that has all the necessary packags for both Open Pose as well as the ASR, LLM, and TTS dependencies needed for my chatbot. I took my virtual environment that I was working in and piped my pip list into a requirements document that could be easily parsed for the installations on the Raspberry pi. While going through the text file, some of the packages were not supported/couldnt be installed correctly due to differences in the hardware and the internal packages that are already used by the hardware. Such as the software Rust beeing needed to install the SudachiPy, as well as needing the portaudio dependency and the libasound dependency for pyaudio and simpleaudio. Once the virtual enviornment was setup, I then went to testing my previously working chatbot to make sure it was all compatible on the raspberry pi and to check if there was additonal prepping needed before the script would work. Luckily the file was able to run with no error, but the LLM model was giving extremely odd responses all of a sudden. I went back to the hugging face repository to check if there were any minor changes that I could make and attempted to use a specific format for prompting that was being used in example code. This prompting format has 3 different sections, a system prompt, user prompt, and asistant sections. The system prompt had different types of guidelines that the response should adhear to while the user prompt was the actual question that the LLM is asked to respond to. The assistant section, I believe is there for the LLM to know that the prompt is over, and that it is its job to complete the response as the assistant. Once this prompting format was put into place, the responses became much more accurate and reasonable, as well as processing much faster. Tommorrow I hope to attempt running both the pose calculations as well as the chatbot to test the capabilities of the raspberry pi and to see if there are any further optimizations that need to be done for both features of the project to be implemented and ran at the same time. 

### Day 2:
Today me and Aaron wanted to work on running both the Ultralytics open pose as well as the conversational chatbot on the raspberry pi. This was to see if there were any conflicts that running both of our files at the same time would run into though there was little to no problems with the running. To have the files both running at the same time we decided to run the open pose model on its own thread as it calculates the angles of the body as well as having a shared state file, that allows us to push and pull data from the user as they are performing their physical acivity. This idea is intended to keep everything in a discrete location therefore we know exaclty where to look when working on any of the files for documentation, as any information that would be needed by both the chatbot and openpose would be pushed to the shared state and could be handled in their own way in their own files. This additionally gives me and Aaron both the freedom to continue working on the pieces that we have developed on our own before coming together to combine them. Finally towards the end of today I attempted to start implementing an audio streaming funciton, so that instead of the file listening for a determined duration, it would intead listen until the user starts talkng and keep track of what the using is saying until completion, procedurally generating the text as it is spoken. Doing this allows for more seemless interaction between the chatbot and the user as well as the potential for a more interactive stop and pause, as currently the user has to wait for the end of that recording period for the script to know when to stop or wait. Additionally we had the idea of having the thread of the Pose detection run on a thread that is asleep until the actual activity or motion is decided to avoid startup times when the activity is decided as well as free up processing power that might be needed if the user is just having a general conversation with the LLM. 

### Day 3:
Today I focused on being able to stream audio in from the user to prevent overall system down time, as well as putting the Pose detection on a thread that could be started and stopped when a user wishes to perform an exercise or stop that exercise. Firstly to start this, I had to change the chat loop that I had originally been working with to implement a callback so that I could continually stream in audio while working with that audio at the sime time for the Vosk ASR. Once this was working as intended I had implemented keyword searches so that as the user is talking live it will detect keywords for stopping or pausing as if a user is in the middle of an exercise and need to stop right away we didn't want the user to have to wait on system down time to process that. Additoinally, I created a dictionary that can be used to store information on the different exercises or therapy activites that would be offered. This was to help keep consistency and formatting to then be able to parse the users audio input for keywords relating to the different activities for a more real-time experience. This dictionary holds the name of the exercies, the muscles it works on/with, as well as a short descrption of how to perform the motion. Once I had detection for what was considered an exercise I then wanted to parse for a numerical value spoken by the user to determine weather the user was seeking explaination or actually wanted to perform an exercise. For example, a user saying "What is a bicep curl" would find the keyword "bicep curl" and the explaination fo the motion would be played, wheras if the user said "I want to do five bicep curls" the program would then process that as wanting to start the bicep exercise for 5 repetitions. Finally, now that I had determined what would be the start condition for an exercise, I know where to start and stop the thread for pose detection. Currently we have parameters hard coded into the pose thread, and we intend to use the shared state data structure that we are using for communication between the chatbot and pose detection to alter the exercise that is being tracked in the pose file for the ability to perform multiple different exercises in a single execution of the program.

### day 4:
Today we focused on the communicatoin between the pose detection and the chatbot files, and what one of the files may need communicated from the other. This was to have a plan to approach our exercise setup and completion to make sure that the user was performing the exercise correctly as well as maintaining a live feedback loop that way the user can be corrected as they complete the exercise. This would all be happening while the user is still able to communicate with the LLM as the calculations are done elsewhere and the chatbot file deals with the flags for when the system should be talking to the user to correct them as well as taking their input in some cases. For example we added a check that if the user goes a certain amount of repetitions not reaching the full range of motion of the exercise that the system will then prompt them if they would like to make the range of motion easier. The user can then respond with yes or no depending on their prefrence and the pose detection will then calulate a rep at an easier or the same range of motion. Additionally if the user is determined to have bad form based off of the angles calculated from their body, the shared data will update a list of "bad forms" so that I can iterate through the areas of bad form correcting the user as I iterate through the different corrections that have an adjustment dialog in their dicitonary. Once we were both finished implementing the getters and setters that we needed for communication we then went to test and debug anything that we could as well as started planning for further improvements that could be made using the shared state. 